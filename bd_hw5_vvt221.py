# -*- coding: utf-8 -*-
"""bd_hw5_vvt221.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVGgS7arU08pcBDqafPXTaR-nbsVT-hE
"""

from pyspark import SparkContext
import sys
sc = SparkContext()



def mapBoroNeigh(shapefile):

  import geopandas as gpd
  
  gdf = gpd.read_file(shapefile)
  
  for neighbor_id,boroname in enumerate(gdf.borough):
    yield(neighbor_id,boroname)

def createIndex(shapefile):
    import rtree
    import fiona.crs
    import geopandas as gpd
    # neighborhood_shp = sc.textFile(shapefile)
    zones = gpd.read_file(shapefile).to_crs(fiona.crs.from_epsg(2263))
    index = rtree.Rtree()
    for idx,geometry in enumerate(zones.geometry):
        
        index.insert(idx, geometry.bounds)
    return (index, zones)

def findZone(
    p, index, zones):
    match = index.intersection((p.x, p.y, p.x, p.y))
    for idx in match:
        if zones.geometry[idx].contains(p):
            return idx
    return None
  
  
def processTrips(pid, records):
    import csv
    import pyproj
    import shapely.geometry as geom
    
    proj = pyproj.Proj(init="epsg:2263", preserve_units=True)    
    index, zones = createIndex(file_1)   
    
    
    if pid==0:
        next(records)
    reader = csv.reader(records)
    counts = {}
    
    for row in reader:
        #try:
        #print(row[5])
        if len(row) == 18:
           p = geom.Point(proj(float(row[5]), float(row[6])))
           zone = findZone(p, index, zones)
           if zone:
              counts[zone] = counts.get(zone,0) + 1
   
        #except ValueError:
            ##print(row[3], row[2])
        #zone = findZone(p, index, zones)
        #if zone:
        #    counts[zone] = counts.get(zone, 0) + 1
    return counts.items()
            

    
    
    
file_1 = "neighborhoods.geojson"
file_2 = sys.argv[1]
print(file_2)

index,zones = createIndex(file_1)  
boro_neigh  = mapBoroNeigh(file_1)
boro_neigh_rdd = sc.parallelize(boro_neigh)
rdd = sc.textFile(file_2)
counts = rdd.mapPartitionsWithIndex(processTrips) \
            .reduceByKey(lambda x,y: x+y)

newRdd = counts.map(lambda x:(x[0],x[1])).join(boro_neigh_rdd.map(lambda x:(x[0],(x[1]))))
newRdd_order = newRdd.map(lambda x:(x[1][1], (x[0], x[1][0])))

rdd_manhattan = newRdd_order.filter(lambda x: x[0] == 'Manhattan')
rdd_brooklyn = newRdd_order.filter(lambda x: x[0] == 'Brooklyn')
rdd_bronx = newRdd_order.filter(lambda x: x[0] == 'Bronx')
rdd_queens = newRdd_order.filter(lambda x: x[0] == 'Queens')
rdd_si = newRdd_order.filter(lambda x: x[0] == 'Staten Island')

rdd_brooklyn_sort = rdd_brooklyn.sortBy(lambda x: x[1][1],ascending=False).collect()[0:3]
rdd_manhattan_sort = rdd_manhattan.sortBy(lambda x: x[1][1],ascending=False).collect()[0:3]
rdd_bronx_sort = rdd_bronx.sortBy(lambda x: x[1][1],ascending=False).collect()[0:3]
rdd_queens_sort = rdd_queens.sortBy(lambda x: x[1][1],ascending=False).collect()[0:3]
rdd_si_sort = rdd_si.sortBy(lambda x: x[1][1],ascending=False).collect()[0:3]

countsPerNeighborhood_brooklyn = list(map(lambda x: (zones['neighborhood'][x[1][0]], x[1][1]), rdd_brooklyn_sort))
countsPerNeighborhood_bronx= list(map(lambda x: (zones['neighborhood'][x[1][0]], x[1][1]), rdd_bronx_sort))
countsPerNeighborhood_queens= list(map(lambda x: (zones['neighborhood'][x[1][0]], x[1][1]), rdd_queens_sort))
countsPerNeighborhood_si= list(map(lambda x: (zones['neighborhood'][x[1][0]], x[1][1]), rdd_si_sort))
countsPerNeighborhood_manhattan = list(map(lambda x: (zones['neighborhood'][x[1][0]], x[1][1]), rdd_manhattan_sort))


print('Brooklyn')
print(countsPerNeighborhood_brooklyn)
print('Bronx')
print(countsPerNeighborhood_bronx)
print('Queens')
print(countsPerNeighborhood_queens)
print('Staten Island')
print(countsPerNeighborhood_si)
print('Manhattan')
print(countsPerNeighborhood_manhattan)
